{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical                  # Converts a class vector (integers) to binary class matrix.\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential                     # A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm           # provides a simple and convenient way to add progress bars to loops and iterable objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Dense Layer (Fully Connected Layer)**:\n",
    "   - The `Dense` layer is a fundamental building block in neural networks. It's also known as a fully connected layer.\n",
    "   - Each neuron in this layer is connected to every neuron in the previous and subsequent layers.\n",
    "   - It's used for learning complex patterns and relationships in the data.\n",
    "   - Common activation functions used with `Dense` layers include **ReLU (Rectified Linear Unit)**, **sigmoid**, and **softmax**.\n",
    "   - Useful for tasks like classification and regression.\n",
    "\n",
    "2. **Conv2D Layer (2D Convolutional Layer)**:\n",
    "   - The `Conv2D` layer performs 2D convolution on input data (usually images).\n",
    "   - It uses a set of learnable filters (kernels) to extract features from local regions of the input.\n",
    "   - Useful for tasks like image recognition, object detection, and segmentation.\n",
    "   - Often followed by activation functions like ReLU.\n",
    "\n",
    "3. **Dropout Layer**:\n",
    "   - The `Dropout` layer helps prevent overfitting by randomly setting a fraction of input units to zero during training.\n",
    "   - It encourages the network to learn robust features by preventing reliance on specific neurons.\n",
    "   - Dropout reduces the risk of overfitting and improves generalization.\n",
    "\n",
    "4. **Flatten Layer**:\n",
    "   - The `Flatten` layer converts multidimensional input (e.g., feature maps from convolutional layers) into a 1D vector.\n",
    "   - It's typically used to transition from convolutional layers to fully connected layers.\n",
    "   - Necessary when passing data from convolutional layers to a dense layer.\n",
    "\n",
    "5. **MaxPooling2D Layer**:\n",
    "   - The `MaxPooling2D` layer downsamples feature maps by selecting the maximum value within a local region (pooling window).\n",
    "   - Helps reduce spatial dimensions while retaining important features.\n",
    "   - Commonly used after convolutional layers to reduce computational complexity and improve translation invariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(dir):\n",
    "    \"\"\"\n",
    "    Creates a dataframe containing image paths and corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        dir (str): The directory path containing subdirectories of labeled images.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - image_paths (list): List of image file paths.\n",
    "            - labels (list): List of corresponding labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for image_name in os.listdir(os.path.join(dir, label)):\n",
    "            image_paths.append(os.path.join(dir, label, image_name))\n",
    "            labels.append(label)\n",
    "        print(f'{dir}/{label} loaded')\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/train/angry loaded\n",
      "images/train/disgust loaded\n",
      "images/train/fear loaded\n",
      "images/train/happy loaded\n",
      "images/train/neutral loaded\n",
      "images/train/sad loaded\n",
      "images/train/surprise loaded\n",
      "images/test/angry loaded\n",
      "images/test/disgust loaded\n",
      "images/test/fear loaded\n",
      "images/test/happy loaded\n",
      "images/test/neutral loaded\n",
      "images/test/sad loaded\n",
      "images/test/surprise loaded\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = create_dataframe(TRAIN_DIR)   # Creating colums in the pandas dataframe\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = create_dataframe(TEST_DIR)   # Creating colums in the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                image     label\n",
      "0            images/train\\angry\\0.jpg     angry\n",
      "1            images/train\\angry\\1.jpg     angry\n",
      "2           images/train\\angry\\10.jpg     angry\n",
      "3        images/train\\angry\\10002.jpg     angry\n",
      "4        images/train\\angry\\10016.jpg     angry\n",
      "...                               ...       ...\n",
      "28816  images/train\\surprise\\9969.jpg  surprise\n",
      "28817  images/train\\surprise\\9985.jpg  surprise\n",
      "28818  images/train\\surprise\\9990.jpg  surprise\n",
      "28819  images/train\\surprise\\9992.jpg  surprise\n",
      "28820  images/train\\surprise\\9996.jpg  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              image     label\n",
      "0       images/test\\angry\\10052.jpg     angry\n",
      "1       images/test\\angry\\10065.jpg     angry\n",
      "2       images/test\\angry\\10079.jpg     angry\n",
      "3       images/test\\angry\\10095.jpg     angry\n",
      "4       images/test\\angry\\10121.jpg     angry\n",
      "...                             ...       ...\n",
      "7061  images/test\\surprise\\9806.jpg  surprise\n",
      "7062  images/test\\surprise\\9830.jpg  surprise\n",
      "7063  images/test\\surprise\\9853.jpg  surprise\n",
      "7064  images/test\\surprise\\9878.jpg  surprise\n",
      "7065   images/test\\surprise\\993.jpg  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    \"\"\"\n",
    "    Extracts features from a list of images.\n",
    "\n",
    "    Args:\n",
    "        images (pd.DataFrame): A DataFrame containing image paths.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of image features. (Returns a 4D array of shape (num_images, 48, 48, 1))\n",
    "    \"\"\"\n",
    "    \n",
    "    # images is a pd.DataFrame['images']\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, color_mode='grayscale')\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    \n",
    "    # Dimension of the image is 48x48. Since the images are grayscale, the depth is 1\n",
    "    features.reshape(len(features), 48, 48, 1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5f49a37a4c4ee0bcee7875f0ead4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c22ee70b0124af3a1a6b67a91d5802d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features = extract_features(train['image'])\n",
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of the data\n",
    "\n",
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# One-hot encoding in machine learning is the conversion of categorical information into a format that may be fed into machine learning algorithms to \n",
    "# improve prediction accuracy. One-hot encoding is a common method for dealing with categorical data in machine learning.\n",
    "\n",
    "# One-hot encoding in machine learning is the conversion of categorical information into a format that may be fed into\n",
    "# machine learning algorithms to improve prediction accuracy. One-hot encoding is a common method for dealing with categorical data in machine learning.\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(train['label'])  # LabelEncoder.fit() takes a categorical column and converts/maps it to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder.transform() transforms labels to normalized encoding.\n",
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])\n",
    "\n",
    "# https://www.geeksforgeeks.org/python-keras-keras-utils-to_categorical/\n",
    "# a vector which has integers that represent different categories, can be converted into a numpy array (or) a matrix \n",
    "# which has binary values and has columns equal to the number of categories in the data.\n",
    "y_train = to_categorical(y_train, num_classes=7)\n",
    "y_test = to_categorical(y_test, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "model = Sequential()\n",
    "\n",
    "# The first layer is a convolutional layer with 32 filters, each with a size of 3x3, and a ReLU activation function.\n",
    "# These layers are used for feature extraction\n",
    "model.add(Conv2D(128, kernel_size = (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer with 128 neurons and a ReLU activation function.\n",
    "# These are used for classification\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(7, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D Layer:\n",
    "\n",
    "The Conv2D layer stands for 2D convolution. It creates a convolution kernel that slides over the input data (usually an image) to produce a tensor of outputs.\n",
    "\n",
    "- **Filters**: This parameter specifies the number of filters (also known as channels) in the convolution. Each filter learns different features from the input data. In your example, you’ve set it to 128.\n",
    "\n",
    "The filters parameter determines the number of convolutional filters that the layer will learn during training.\n",
    "- Each filter is a small matrix (usually 3x3 or 5x5) that slides over the input data (such as an image) to extract features.\n",
    "- These filters are initialized with small random values and updated during training to minimize the loss.\n",
    "- Over time, the filters learn to detect specific features, such as edges, textures, or patterns.\n",
    "- For example, a filter might specialize in detecting vertical edges, while another focuses on diagonal lines.\n",
    "- The more filters you have, the more diverse features the layer can learn.\n",
    "- Layers early in the network architecture (closer to the input image) typically learn fewer filters, while deeper layers learn more filters12.\n",
    "- So, when you define a Conv2D layer with a specific number of filters (e.g., Conv2D(64, kernel_size=(5, 5), activation='relu')), it means that the layer will learn 64 different filters.  These filters will automatically adapt during training to capture relevant features from the input data.\n",
    "\n",
    "As for the output of the convolutional layer:\n",
    "\n",
    "- If you apply this layer to a 32x32 RGB image, you’ll get 64 feature maps, each of size 28x28 (due to the 5x5 kernel and 1x1 stride).\n",
    "- These feature maps represent the activation of each filter across the input image.\n",
    "- Further layers (e.g., pooling, fully connected) combine information from these feature maps to make predictions or perform other tasks.\n",
    "\n",
    "- **Kernel Size**: Determines the size of the convolution window. It can be specified as an integer or a tuple of two integers. Here, you’ve used a 3x3 kernel.\n",
    "- **Strides**: Specifies the step size for the kernel as it moves across the input. A stride of (1, 1) means it moves one pixel at a time in both dimensions.\n",
    "- **Padding**: Determines how the input is padded before applying the convolution. \"valid\" means no padding, while \"same\" pads the input to maintain the output size.\n",
    "- **Activation**: An optional activation function applied to the output. In your case, it’s ReLU (Rectified Linear Unit), which introduces non-linearity.\n",
    "\n",
    "## MaxPooling2D Layer:\n",
    "\n",
    "The MaxPooling2D layer performs max pooling on the input data.\n",
    "- **Pool Size**: Specifies the size of the pooling window. Here, it’s (2, 2), meaning it takes the maximum value from a 2x2 region.\n",
    "Max pooling reduces the spatial dimensions of the feature maps, helping to capture important features while reducing computation.\n",
    "\n",
    "## Dropout Layer:\n",
    "\n",
    "The Dropout layer helps prevent overfitting by randomly setting a fraction of input units to zero during training.\n",
    "The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks.\n",
    "- **Dropout Rate**: You’ve set it to 0.4, meaning 40% of the input units will be dropped during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer: **Adam**\n",
    "Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "\n",
    "### Loss: **Categorical Crossentropy**\n",
    "We utilize categorical cross-entropy loss in multi-class classification tasks with more than two mutually exclusive classes. Similarly to the binary, this type of cross-entropy loss function quantifies the dissimilarity between the predicted probabilities and the true categorical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 128)       1280      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 23, 23, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 23, 23, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 21, 21, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 10, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 10, 256)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 1, 1, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,232,199\n",
      "Trainable params: 4,232,199\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "226/226 [==============================] - 46s 121ms/step - loss: 1.8223 - accuracy: 0.2429 - val_loss: 1.8037 - val_accuracy: 0.2583\n",
      "Epoch 2/50\n",
      "226/226 [==============================] - 23s 100ms/step - loss: 1.7984 - accuracy: 0.2508 - val_loss: 1.7719 - val_accuracy: 0.2613\n",
      "Epoch 3/50\n",
      "226/226 [==============================] - 23s 102ms/step - loss: 1.7236 - accuracy: 0.2917 - val_loss: 1.6515 - val_accuracy: 0.3312\n",
      "Epoch 4/50\n",
      "226/226 [==============================] - 23s 103ms/step - loss: 1.6138 - accuracy: 0.3604 - val_loss: 1.4386 - val_accuracy: 0.4462\n",
      "Epoch 5/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.5001 - accuracy: 0.4145 - val_loss: 1.3891 - val_accuracy: 0.4679\n",
      "Epoch 6/50\n",
      "226/226 [==============================] - 24s 104ms/step - loss: 1.4367 - accuracy: 0.4487 - val_loss: 1.3020 - val_accuracy: 0.5058\n",
      "Epoch 7/50\n",
      "226/226 [==============================] - 25s 109ms/step - loss: 1.3895 - accuracy: 0.4680 - val_loss: 1.2605 - val_accuracy: 0.5200\n",
      "Epoch 8/50\n",
      "226/226 [==============================] - 25s 113ms/step - loss: 1.3533 - accuracy: 0.4757 - val_loss: 1.2423 - val_accuracy: 0.5283\n",
      "Epoch 9/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.3305 - accuracy: 0.4893 - val_loss: 1.2114 - val_accuracy: 0.5384\n",
      "Epoch 10/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.2991 - accuracy: 0.5023 - val_loss: 1.1924 - val_accuracy: 0.5447\n",
      "Epoch 11/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.2846 - accuracy: 0.5086 - val_loss: 1.1901 - val_accuracy: 0.5535\n",
      "Epoch 12/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.2640 - accuracy: 0.5156 - val_loss: 1.1807 - val_accuracy: 0.5522\n",
      "Epoch 13/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.2471 - accuracy: 0.5258 - val_loss: 1.1399 - val_accuracy: 0.5725\n",
      "Epoch 14/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.2340 - accuracy: 0.5313 - val_loss: 1.1401 - val_accuracy: 0.5727\n",
      "Epoch 15/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.2238 - accuracy: 0.5306 - val_loss: 1.1365 - val_accuracy: 0.5713\n",
      "Epoch 16/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.2111 - accuracy: 0.5386 - val_loss: 1.1362 - val_accuracy: 0.5699\n",
      "Epoch 17/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.1961 - accuracy: 0.5438 - val_loss: 1.1108 - val_accuracy: 0.5868\n",
      "Epoch 18/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.1842 - accuracy: 0.5470 - val_loss: 1.1132 - val_accuracy: 0.5852\n",
      "Epoch 19/50\n",
      "226/226 [==============================] - 24s 108ms/step - loss: 1.1687 - accuracy: 0.5557 - val_loss: 1.1043 - val_accuracy: 0.5882\n",
      "Epoch 20/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.1612 - accuracy: 0.5542 - val_loss: 1.1159 - val_accuracy: 0.5815\n",
      "Epoch 21/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.1559 - accuracy: 0.5641 - val_loss: 1.0879 - val_accuracy: 0.5961\n",
      "Epoch 22/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.1437 - accuracy: 0.5660 - val_loss: 1.0926 - val_accuracy: 0.5955\n",
      "Epoch 23/50\n",
      "226/226 [==============================] - 25s 112ms/step - loss: 1.1316 - accuracy: 0.5717 - val_loss: 1.0874 - val_accuracy: 0.5909\n",
      "Epoch 24/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.1268 - accuracy: 0.5747 - val_loss: 1.0801 - val_accuracy: 0.5926\n",
      "Epoch 25/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.1154 - accuracy: 0.5734 - val_loss: 1.0734 - val_accuracy: 0.5969\n",
      "Epoch 26/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.1130 - accuracy: 0.5770 - val_loss: 1.0742 - val_accuracy: 0.6006\n",
      "Epoch 27/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.1034 - accuracy: 0.5825 - val_loss: 1.0748 - val_accuracy: 0.6030\n",
      "Epoch 28/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0905 - accuracy: 0.5858 - val_loss: 1.0573 - val_accuracy: 0.6068\n",
      "Epoch 29/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.0906 - accuracy: 0.5908 - val_loss: 1.0708 - val_accuracy: 0.6059\n",
      "Epoch 30/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0772 - accuracy: 0.5917 - val_loss: 1.0591 - val_accuracy: 0.6104\n",
      "Epoch 31/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0699 - accuracy: 0.5955 - val_loss: 1.0559 - val_accuracy: 0.6093\n",
      "Epoch 32/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0664 - accuracy: 0.5980 - val_loss: 1.0677 - val_accuracy: 0.6043\n",
      "Epoch 33/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0565 - accuracy: 0.6009 - val_loss: 1.0694 - val_accuracy: 0.6059\n",
      "Epoch 34/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.0476 - accuracy: 0.6013 - val_loss: 1.0755 - val_accuracy: 0.6002\n",
      "Epoch 35/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0474 - accuracy: 0.6052 - val_loss: 1.0478 - val_accuracy: 0.6141\n",
      "Epoch 36/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0358 - accuracy: 0.6055 - val_loss: 1.0647 - val_accuracy: 0.5998\n",
      "Epoch 37/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0353 - accuracy: 0.6089 - val_loss: 1.0477 - val_accuracy: 0.6080\n",
      "Epoch 38/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0233 - accuracy: 0.6162 - val_loss: 1.0357 - val_accuracy: 0.6153\n",
      "Epoch 39/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.0210 - accuracy: 0.6166 - val_loss: 1.0379 - val_accuracy: 0.6090\n",
      "Epoch 40/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 1.0150 - accuracy: 0.6190 - val_loss: 1.0325 - val_accuracy: 0.6124\n",
      "Epoch 41/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 1.0097 - accuracy: 0.6197 - val_loss: 1.0410 - val_accuracy: 0.6168\n",
      "Epoch 42/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 0.9996 - accuracy: 0.6236 - val_loss: 1.0448 - val_accuracy: 0.6180\n",
      "Epoch 43/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 0.9939 - accuracy: 0.6282 - val_loss: 1.0463 - val_accuracy: 0.6142\n",
      "Epoch 44/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 0.9950 - accuracy: 0.6229 - val_loss: 1.0440 - val_accuracy: 0.6127\n",
      "Epoch 45/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 0.9819 - accuracy: 0.6337 - val_loss: 1.0415 - val_accuracy: 0.6162\n",
      "Epoch 46/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 0.9753 - accuracy: 0.6346 - val_loss: 1.0398 - val_accuracy: 0.6202\n",
      "Epoch 47/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 0.9718 - accuracy: 0.6359 - val_loss: 1.0627 - val_accuracy: 0.6098\n",
      "Epoch 48/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 0.9668 - accuracy: 0.6379 - val_loss: 1.0394 - val_accuracy: 0.6200\n",
      "Epoch 49/50\n",
      "226/226 [==============================] - 24s 106ms/step - loss: 0.9582 - accuracy: 0.6425 - val_loss: 1.0416 - val_accuracy: 0.6145\n",
      "Epoch 50/50\n",
      "226/226 [==============================] - 24s 107ms/step - loss: 0.9626 - accuracy: 0.6400 - val_loss: 1.0341 - val_accuracy: 0.6227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225abb619f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=50, validation_data=(x_test, y_test))\n",
    "\n",
    "# Epoch 10: Accuracy 0.55\n",
    "# Epoch 50: Accuracy 0.62\n",
    "# Epoch 70: Accuracy 0.6279\n",
    "# Log: Model crashed after 72 epochs, retraining for 50 epochs since no notable progress made after additional epochs\n",
    "# Epoch 50: Accuracy 0.6227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "json = open('emotiondetector.json', 'r')\n",
    "model_json = json.read()\n",
    "json.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('emotiondetector.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Angry, 1: Disgust, 2: Fear, 3: Happy, 4: Neutral, 5: Sad, 6: Surprise\n",
    "label = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(image):\n",
    "    \"\"\"\n",
    "    Extracts features from an image.\n",
    "\n",
    "    Args:\n",
    "        image (str): The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of image features. (Extracting a single image)\n",
    "    \"\"\"\n",
    "    img = load_img(image, color_mode='grayscale')\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1, 48, 48, 1)\n",
    "    return feature/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 60ms/step\n",
      "sad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x226b3207ee0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = 'images/test/angry/65.jpg'\n",
    "img = ef(test_image)\n",
    "\n",
    "# Predicting with the model\n",
    "pred = model.predict(img)\n",
    "pred_label = label[np.argmax(pred)]\n",
    "print(pred_label)\n",
    "plt.imshow(load_img(test_image))\n",
    "# Predicts 'sad'\n",
    "# I mean, can you really blame it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
